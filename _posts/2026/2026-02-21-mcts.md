---
title: "MCTS에 대해서 알아보기"
category: RL
tags: [인공지능, RL, 구현]
comment: true
---

### Introduction

Monte Carlo Tree Search (MCTS)란 무작위 추출에 기초해 탐색 트리를 확장하는 알고리즘이다. MCTS는 다른 트리 탐색과는 다르게 무작위 샘플링을 통해 시뮬레이션을 진행하고, 그 과정에서 얻은 Reward를 바탕으로 트리를 갱신하면서 최종적으로 어떤 선택지가 더 유망한 결과를 가져오는 지에 대해 알려주는 역할을 한다. 우리에게 익숙한 용례로는 알파고와 같이 보드게임 인공지능에 주로 사용된다.

### Concept

![MCTS](/post_image/2026/0221/image.png)

MCTS는 4 가지 단계가 존재하고 이를 반복하여 트리를 구성할 수 있다.

#### Traversal (Selection)

루트 노드부터 시작해 가장 유망한 결과를 가져오는 자식 노드를 탐색하고 선택하는 과정이다. Upper Confidence Bound1(UCB1) 공식을 통해서 각 노드들을 평가하고 결과값이 가장 큰 노드를 선택하는 과정을 반복한다. UCB1의 공식은 다음과 같다.

$$
UCB1(s_i) = \bar{v_i} + c\sqrt{\frac{\ln{N}}{n_i}}
$$

$s_i$는 선택된 노드, $\bar{v_i}$는 선택된 노드의 가치, ${n_i}$는 방문 횟수를 의미한다.  
UCB1은 가장 가치가 높은 노드를 선택하되 탐색도 함께 진행할 수 있도록 한다. 만약 방문한 횟수가 적은 노드라면 더 탐색할 수 있도록 하며, $c$의 값을 키우면 탐색의 비중을 더 높일 수 있다.

#### Expansion

Expansion 단계는 Leaf Node에서는 선택할 자식 노드가 없음으로 새로운 자식 노드를 만들어 내는 단계다. Traversal 과정을 거치면서 Leaf Node에 도달했을 때 **이전에 방문해본 경험이 있는 노드**라면 진행한다.

#### Rollout (Random Simulation)

**처음 방문하는 Leaf Node**에 도달하게 되었을 때 수행하는 단계이다. Rollout은 무작위로 Action을 수행해 환경이 종료될 때까지 플레이를 시뮬레이션하는 과정이다. 종료까지 시뮬레이션을 진행하면 Reward가 주어지고 이 Reward를 바탕으로 Backpropation 과정을 진행한다.

#### Backpropagation

Terminal Node에 도착하거나 Rollout을 수행해 Reward를 획득했을 때 수행하는 단계이다. 루트 노드까지 올라가며 얻은 Reward를 경로 상의 모든 노드들의 Value에 더해준다.

사실 글로만 본다면 크게 와닿지 않을 것이다. CS 전공인만큼 코드로 한 번 살펴보자!


### Implementation
``` python
class Node:
    def __init__(self, env):
        self.state = env
        self.n = 0
        self.v = 0
        self.parent = None
        self.childs = []
        self.is_terminal = False
        self.action = None
        self.reward = 0

    def get_ucb(self, c=2):
        if self.n == 0:
            return float('inf')

        q = self.v / self.n
        u = c * np.sqrt(np.log(self.parent.n) / self.n)
        return q + u
```


제일 먼저 MCTS를 구현하기 이전에 노드에 대해서 클래스를 정의해준다. 각각 노드가 저장해야 하는 값들을 초기화해주고, 해당 노드의 ucb를 계산할 수 있도록 메소드를 정의해준다. 이 때 주의해야할 점은 방문 횟수가 0이라면 Zero Division이 발생함으로 예외 처리로 Inf를 반환하도록 해 반드시 그 노드가 선택되도록 해주자.


``` python
class MCTS:
    def __init__(self, env):
        self.root = Node(env.copy())
```

MCTS의 초기 상태는 단순히 Env를 입력받아 루트 노드를 생성해주는 것만으로도 끝난다. 이후에는 Traversal, Expansion, Rollout, Backpropagation의 과정을 거치면서 트리가 확장된다.

이제 하나하나씩 MCTS의 메소드들을 살펴보자.

``` python
def traversal(self):
    cur = self.root

    # Leaf Node 찾기
    while cur.childs: 
        cur = self._select_node(cur)

    # Terminal Node에 도달 시 바로 Backpropagation
    if cur.is_terminal:
        reward = cur.reward

    # 처음 방문한 노드일 때 Rollout 수행
    elif cur.n == 0:
        reward = self.rollout(cur)

    # 이미 방문해본 노드일 때 Expansion 수행
    else:
        self.expansion(cur)
        cur = cur.childs[0]
        reward = cur.reward if cur.is_terminal else self.rollout(cur)

    self.backpropagation(cur, reward)

def _select_node(self, cur):
    max_ucb = float('-inf')
    target = None

    for node in cur.childs:
        ucb = node.get_ucb()
        if ucb > max_ucb:
            target = node
            max_ucb = ucb

    return target
```

Traversal 단계의 구현이다. root부터 시작해서 Leaf Node를 찾을 때까지 탐색한다. 노드를 선택하는 과정에서는 자식 노드 중에 UCB의 값이 가장 큰 노드를 선택하도록 한다.

Leaf Node에 도달한 이후에는 세 가지 경우가 생긴다.

첫 번째로 Terminal Node에 도착한 경우다. Terminal Node는 Leaf Node와 헷갈릴 수 있는데 다른 개념이다. Leaf Node는 단순히 자식 노드가 없는 노드이지만, Terminal Node는 환경 자체가 끝나서 Reward가 정산된 상태의 노드다. Terminal Node의 경우에는 이미 Reward를 알고 있으니 바로 해당 노드의 Reward 값을 가져온다.

두 번째로는 처음 방문한 노드인 경우다. 이 경우에는 바로 Rollout을 수행해 Reward를 계산할 수 있도록 한다.

마지막으로는 이미 방문했지만 Terminal Node는 아닌 경우이다. 이 때는 바로 Rollout을 수행하지 않고 Expansion을 수행해 자식 노드들을 생성한 뒤, 자식 노드 중 하나를 선택해 Rollout을 수행해서 Reward를 계산한다. 이 때 흔히 구현 과정에서 실수할 수 있는 경우가 있는데 Expansion 한 직후 자식 노드가 Terminal Node가 되는 경우가 있다. 이 경우에는 Rollout이 불가능함으로 즉시 해당 노드의 Reward를 가져올 수 있도록 한다.

``` python
def expansion(self, node):
    all_actions = node.state.get_valid_actions()
    all_action_indices = np.where(all_actions == 1)[0]

    for action in all_action_indices:
        state = node.state.copy()
        next_state, reward, done, info = state.step(action)
        new_node = Node(state.copy())
        new_node.action = action
        new_node.parent = node
        node.childs.append(new_node)

        # Terminal Node인 경우
        if done:
            new_node.is_terminal = True
            new_node.reward = reward
```

``` python

def rollout(self, node):
    env = node.state.copy()

    while not env.done:
        all_actions = env.get_valid_actions()
        random_action = np.random.choice(np.where(all_actions == 1)[0])
        next_state, reward, done, info = env.step(random_action)

    return reward
```

여기서 헷갈릴 수 있는 점이 한 가지 있다. 위 과정에서 next_state 대신 env 자체를 복사해서 step() 하는 것을 확인할 수 있는데 next_state의 경우에는 단순히 numpy의 상태 배열이기 때문에 실제로 MCTS가 시뮬레이션을 진행하려면 상태가 아닌 전체 환경이 필요함으로 하나의 env를 계속 step 하는 방식으로 구현해야 한다!

이렇게 Traversal 단계에서 UCB가 높은 노드를 선정하고 Expansion과 Rollout을 수행하여 Reward를 얻어냈다. 이 과정 이후에는 Backpropagation을 수행한다.

``` python
def backpropagation(self, node, reward):
    cur = node
    while cur is not None:
        cur.v += reward
        cur.n += 1
        cur = cur.parent
```

Backpropagation도 어렵지 않게 Root 위로 올라가면서 갱신하도록 구현할 수 있다.

최종적으로 Traversal을 반복하게 되면 Selection, Expansion, Rollout, Backpropagation이 수행되면서 MCTS가 확장되게 된다!

``` python
def get_action(self, num_simulations=1000):
    for _ in range(num_simulations):
        self.traversal()

    best_child = max(self.root.childs, key=lambda c: c.n)
    return best_child.action
```

결과적으로 ROOT 입장에서 자식 노드 중에서 가장 방문 횟수가 높은 것을 선택하게 되면 그 행동이 가장 유망한 행동이 되는 것이다!

이 때 best_child를 선정할 때 왜 UCB나 Value 기준이 아닌 방문 횟수로 선정해야 할까? 그 이유는 UCB는 방문 횟수가 적으면 탐색 보정이 들어가기 때문에 항상 최선의 수를 선택한다는 보장이 없고, Value의 경우에는 그 수가 실제로는 악수지만 방문 횟수가 적어 운 좋게 Reward를 높게 받은 것과 같이 왜곡된 정보도 존재하기 때문이다. 그렇기 때문에 Traversal 과정에서 제일 선택을 많이 되었다는 것은 그 노드가 UCB가 높은 경우가 많았다는 의미이기 때문에 신뢰도가 높음으로 방문 횟수를 기준으로 선택한다.

### In Two-ZeroSum Game

사실 대부분의 사람들은 나와 마찬가지로 MCTS를 보드게임 AI 구현을 위해 공부하는 경향이 클 것이다. 그렇다면 체스, 바둑과 같이 한 사람의 승리가 다른 한 사람의 패배로 직결되는 상황이라면 어떻게 MCTS를 구현해야 할까? 단순히 플레이어마다 MCTS를 만들기에는 너무 낭비가 심하기 때문에 공통으로 사용할 수 있는 트리를 만들어야 한다!

이를 구현하기 위해서는 Reward를 조금만 수정해주면 된다.

``` python
    def rollout(self, node):
        env = node.state.copy()

        while not env.done:
            all_actions = env.get_valid_actions()
            random_action = np.random.choice(np.where(all_actions == 1)[0])
            next_state, reward, done, info = env.step(random_action)

        # Rollout을 시작한 시점의 Player를 기준으로 Reward를 갱신
        # 해당 수를 둔 플레이어가 승리할 시 1, 패배할 시 -1
        winner = env.winner
        player = node.state.current_player

        if winner == player:
            return 1
        elif winner == -player:
            return -1
        else:
            return 0

```

제일 먼저 Rollout의 경우에는 Reward를 Rollout이 처음 수행된 노드의 플레이어를 기준으로 Reward를 계산해야 한다. 그렇기 때문에 실제 승리자와 Rollout 시작 노드의 플레이어를 비교해서 일치할 시 좋은 Reward를 줄 수 있도록 한다.

``` python
    def backpropagation(self, node, reward):
        cur = node
        while cur is not None:
            cur.v += reward
            cur.n += 1
            cur = cur.parent
            reward = -reward
```

Backpropagation 과정에도 변화가 생긴다. 만약 Leaf Node에서 Rollout을 수행한 후 승리했다고 가정해보자. **해당 노드가 승리하면 부모 노드는 패배**한다. 그렇기 때문에 Backpropataion 과정에서 반드시 **Reward를 반전**시켜주면서 올라가는 과정이 필요하다.

``` python
    def get_ucb(self, c=2):
        if self.n == 0:
            return float('inf')

        q = -self.v / self.n
        u = c * np.sqrt(np.log(self.parent.n) / self.n)
        return q + u
```

마지막으로 UCB를 계산할 때에도 q를 계산할 때 음수를 곱해줘야 한다! **나의 자식 노드의 패배가 결국 나의 승리로 직결**되는 것이기 때문에 음수를 곱해줘야만 올바르게 유먕한 수를 결정할 수 있다.

결과적으로 최종 구현 코드는 다음과 같다.

``` python
class Node:
    def __init__(self, env):
        self.state = env
        self.n = 0
        self.v = 0
        self.parent = None
        self.childs = []
        self.is_terminal = False
        self.action = None
        self.reward = 0

    def get_ucb(self, c=2):
        if self.n == 0:
            return float('inf')

        q = -self.v / self.n
        u = c * np.sqrt(np.log(self.parent.n) / self.n)
        return q + u

class MCTS:
    def __init__(self, env):
        self.root = Node(env.copy())

    def traversal(self):
        cur = self.root

        # Leaf Node 찾기
        while cur.childs: 
            cur = self._select_node(cur)

        # Terminal Node에 도달 시 바로 Backpropagation
        if cur.is_terminal:
            reward = cur.reward

        # 처음 방문한 노드일 때 Rollout 수행
        elif cur.n == 0:
            reward = self.rollout(cur)

        # 이미 방문해본 노드일 때 Expansion 수행
        else:
            self.expansion(cur)
            cur = cur.childs[0]
            reward = cur.reward if cur.is_terminal else self.rollout(cur)

        self.backpropagation(cur, reward)

    def _select_node(self, cur):
        max_ucb = float('-inf')
        target = None

        for node in cur.childs:
            ucb = node.get_ucb()
            if ucb > max_ucb:
                target = node
                max_ucb = ucb

        return target

    def expansion(self, node):
        all_actions = node.state.get_valid_actions()
        all_action_indices = np.where(all_actions == 1)[0]

        for action in all_action_indices:
            state = node.state.copy()
            next_state, reward, done, info = state.step(action)
            new_node = Node(state.copy())
            new_node.action = action
            new_node.parent = node
            node.childs.append(new_node)

            # Expansion 도중 Terminal Node인 경우 바로 Backpropagation
            if done:
                new_node.is_terminal = True
                new_node.reward = -reward

    def rollout(self, node):
        env = node.state.copy()

        while not env.done:
            all_actions = env.get_valid_actions()
            random_action = np.random.choice(np.where(all_actions == 1)[0])
            next_state, reward, done, info = env.step(random_action)

        # Rollout을 시작한 시점의 Player를 기준으로 Reward를 갱신
        # 해당 수를 둔 플레이어가 승리할 시 1, 패배할 시 -1
        winner = env.winner
        player = node.state.current_player

        if winner == player:
            return 1
        elif winner == -player:
            return -1
        else:
            return 0

    def backpropagation(self, node, reward):
        cur = node
        while cur is not None:
            cur.v += reward
            cur.n += 1
            cur = cur.parent
            reward = -reward

    def get_action(self, num_simulations=1000):
        for _ in range(num_simulations):
            self.traversal()

        best_child = max(self.root.childs, key=lambda c: c.n)
        return best_child.action
```

최종적으로 하나의 MCTS를 만들어서 양 플레이어 모두가 공동으로 최선의 수가 무엇인지 판단할 수 있다!


### Discussion

MCTS에 대해서 이야기했지만 실제로 이 코드와 함께 AI 성능을 측정해보면 그렇게 성능이 좋지 않다.. Rollout 자체가 결국 무작위로 수를 두는 것이기 때문에 아무리 Reward를 갱신하더라도 전략 없이 무작위로 수를 두고 그나마 결과가 좋았던 것만 선택하는 것이기 때문이다. 그것이 진짜 최선의 수인지, 전망이 좋은 수인지는 정확하게 알지 못한다. 아무리 Traversal 횟수를 높이더라도 그 막대한 상태 공간들을 모두 탐색하는 것은 어려울 것이고, 최선의 수를 찾는 것은 어려울 것이다. 그래서 순수 MCTS로는 AI를 만들기 어렵다.

이런 상황에서 이런 무작위 Rollout의 단점을 해결하기 위해 등장한 것이 AlphaGo나 AlphaZero와 같이 신경망을 활용한 방법이다. 무작위 Rollout을 삭제하고, 좀 더 승률이 좋은 곳으로 정책을 판단할 수 있도록 신경망을 구현하게 된다면 더 효율적으로 시뮬레이션을 진행해서 MCTS를 확장할 수 있을 것이다. 이에 대한 자세한 내용은 이후에 AlphaZero 논문을 통해 추가로 다루도록 하겠다.

### Reference
John Levine - Monte Carlo Tree Search: [Youtube Video](https://www.youtube.com/watch?v=UXW2yZndl7U)